
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>tensorflow2.0 YOLOv3</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="yolo"
                  title="tensorflow2.0 YOLOv3"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="初めに" duration="10">
        <p>本スライドは、物体検出のアルゴリズムの1つ、YOLOv3をtensorflow2.0で試す手順をまとめたものです。</p>
<p>もちろん既存のデータセットではなく、みなさんが用意したデータを学習させて物体検出を行います。</p>
<p>以下の画像は、「グー」「チョキ」「パー」の手の形を学習させて検出させている様子です。画像などを用意するのは単純作業で辛いですが、上手く検出できるとその反面嬉しさも倍増するので頑張りましょう。</p>
<p class="image-container"><img alt="img1" src="img/c678f23ac12099bb.jpeg"></p>
<h2 is-upgraded>今回使用するサービスやツール</h2>
<h3 is-upgraded>Google Colaboratory</h3>
<p class="image-container"><img alt="img2" src="img/22b806a5bc948876.jpeg"></p>
<p>Googleが提供する「機械学習/Deep Learningの教育や研究の促進を目的としたプロジェクト」です。ブラウザから操作できるので、環境構築不要です。また、GPUというハイスペックな環境を使用できます。Googleアカウントがない方は取得しておいてください。</p>
<p>Google Colaboratoryで使用するファイルを「ノートブック」と呼び、今回はこちらが用意したノートブックを使用します。</p>
<p class="image-container"><img alt="img3" src="img/ef793e64908cb87d.jpeg"></p>
<p>このように、ノートブックにはプログラミング言語だけではなくドキュメントも記すことができます。</p>
<p class="image-container"><img alt="img4" src="img/ece79246e200bc4a.jpeg"></p>
<p>また、セル単位で実行するコードを分けることができます。環境や変数などは同一ノートブック内では共通されます。</p>
<p>コードを実行するには、実行したいセルにカーソルを合わせた状態で、<code>ctrl</code>+<code>Enter</code>(Macの場合は、<code>cmd</code>+<code>Enter</code>)です。もしくは、再生ボタンをクリックします。</p>
<p>Google Colaboratoryについて気になる方は公式の<a href="https://colab.research.google.com/notebooks/intro.ipynb?hl=ja" target="_blank">説明</a>もご参照ください。</p>
<h3 is-upgraded>ラズベリーパイ</h3>
<p class="image-container"><img alt="img6" src="img/b954fdb177c68798.jpeg"></p>
<p>「ラズベリーパイ」は、必要最低限な機能を備えた小型PCです。その分スペックは低いですが数千円で購入できる手軽さから、子ども用のプログラミング入門キットとしてだけでなく、大学の工学部や情報学部などでの授業に取り入れられてます。また大人も楽しめる趣味として世界中で幅広い層に利用されています。</p>
<p>今回はこのラズパイにカメラをつけて、みなさんの手元でリアルタイム物体検出を行います。</p>
<p>小型なため電力さえあれば設置場所にも困りません。また、ぬいぐるみなどに装着することもできます。</p>
<h3 is-upgraded>VoTT</h3>
<p class="image-container"><img alt="img7" src="img/47ffe6f8574823d3.jpeg"></p>
<p>「VoTT」は、Microsoft社が提供する、動画・画像に対してアノテーションをおこなう無償のアノテーションツールです。</p>
<p>アノテーションとは、対象となるデータに対して正解ラベル（タグ）や対象物の座標等関連する情報を注釈として付与することを指します。たとえば、犬の画像に対して「犬」とラベル付けたり、バウンディングボックスで犬を囲んだりします。</p>
<h2 is-upgraded>大まかな物体検出の流れ</h2>
<ol type="1">
<li>学習データの作成  <ul>
<li>検出したいモノの画像用意</li>
<li>アノテーション作業</li>
</ul>
</li>
<li>学習</li>
<li>推論（検出）</li>
</ol>
<p>オリジナルデータで物体検出をするために、まずは学習データを皆さんに作成してもらいます。</p>
<p>その後、上述したGoogle Colaboratoryで学習を行い、最後にラズパイでリアルタイム画像検出を行います。</p>
<h2 is-upgraded>今回のポイント</h2>
<p>ラズパイで物体検出を簡単に試せるよう、以下の3つの工夫をしました。</p>
<ul>
<li>転移学習</li>
<li>YOLOv3-tinyの採用</li>
<li>画像の水増し</li>
</ul>
<h3 is-upgraded>転移学習</h3>
<p>物体検出は、AIに学習させる時間や必要なデータ数が大量になります。そのため、今回は「転移学習」と呼ばれるものを行い「時間」「データ数」の必要量を減らします。</p>
<p>簡単に言うと、ある領域で学習させたモデルを別の領域に適応させる技術です。事前に学習されているため、学習に時間がかかりません。たとえば、「犬の種類の画像認識を行うモデル」を「猫の種類の画像認識」に応用できます。</p>
<h3 is-upgraded>YOLOv3-tinyの採用</h3>
<p>今回は、YOLOv3-tinyと呼ばれる「正確さよりもリアルタイム性や軽量さを重視したモデル」を使用します。</p>
<p>精度は落ちますが、ラズパイでも動作できる程度に軽いモデルです。</p>
<h3 is-upgraded>学習データの水増し</h3>
<p>画像認識でもよく採用されている「学習データの水増し」を使って、必要な画像枚数を少なくします。また、この後に作成するアノテーションデータ自体の水増しも行右ことでアノテーション作業量を減らします。</p>
<p>「水増し」という言葉の印象はあまりよくありませんが、単純に学習データとなる画像をプログラムを使って複製するだけです。たとえば、元の画像枚数が20枚しかなくても100枚に増やして学習させることができます。</p>
<p>自分が試したところ、1つのクラスに対して20種類ぐらいの画像がを用意すれば上手く検出してくれました。（最終的には、1つのクラスに対して300 ~ 400枚使用していることにあります）</p>
<h2 is-upgraded>事前準備</h2>
<p>いくつか時間のかかる作業があるのでそちらを先に済ませます。</p>
<h3 is-upgraded>ノートブックのコピー</h3>
<p>まずは、今回用意したノートブックをみなさんのGoogle Driveにコピーします。</p>
<p><a href="https://colab.research.google.com/drive/1FzwCZIIsqvSDktRDWVclO_jokT55qoA0?usp=sharing" target="_blank">こちら</a>をクリックして開いてください。</p>
<p class="image-container"><img alt="img8" src="img/685b651aa180d4d2.jpeg"></p>
<p>開いたら「ファイル」 &gt; 「ドライブにコピーを保存」をクリックします。これでみなさんのドライブにコピーが作成されます。</p>
<p class="image-container"><img alt="img9" src="img/d92bee017053c23b.jpeg"></p>
<p>ただし、このコピーししたノートブックは実行できる状態ではないので「ファイル」&gt;「Playgroundモードで開く」をクリックして実行できるようします。</p>
<p>コピーできたら、1つ目のセル「A.Google Colaboratoryテスト用コード」を試しに実行してみましょう！実行するには、<code>ctrl</code>+<code>Enter</code>(Macの場合は、<code>cmd</code>+<code>Enter</code>)です。もしくは、再生ボタンをクリックします。</p>
<p class="image-container"><img alt="img10" src="img/7b0321253f13556a.jpeg"></p>
<p>このようになれば問題ありません。</p>
<h3 is-upgraded>Google Driveとの連携</h3>
<p>無事にノートブックをコピーできて、実行できるようになったら作業を進めていきます。まずは、Google DriveとGoogle Colaboratoryを連携させます。</p>
<p>「B.Google Driveとの連携」を実行しましょう。</p>
<p class="image-container"><img alt="img11" src="img/72c64caa266fcff3.jpeg"></p>
<p>実行すると、このようにリンクが現れるのでクリックして開きます。</p>
<p class="image-container"><img alt="img12" src="img/679685bc8c2703a6.jpeg"></p>
<p>クリックすると、連携先のGoogleアカウントを選びます。複数のアカウントを持っている方は、ノートブックをコピーしたアカウントを選んでください。</p>
<p class="image-container"><img alt="img13" src="img/2977ba487630b6f.jpeg"></p>
<p>「許可」を押します。</p>
<p class="image-container"><img alt="img14" src="img/e2959c19775421ce.jpeg"></p>
<p>赤色で隠されている部分をコピーします。赤枠箇所をクリックすると自動でコピーされます。</p>
<p class="image-container"><img alt="img15" src="img/47d09c439a784bba.jpeg"></p>
<p>コピーできたら、ノートブックに戻ってこの箇所に貼り付けて、<code>Enter</code>を押します。</p>
<p class="image-container"><img alt="img16" src="img/2d24f23e73fdb135.jpeg"></p>
<p>少し時間が経って、このように表示されると連携の終了です。</p>
<p class="image-container"><img alt="img17" src="img/40fcb9b7bea35964.jpeg"></p>
<p>左側のフォルダマークをクリックすると、<code>drive</code>フォルダーが表示され、ここでdrive内のファイルを確認できます。</p>
<h3 is-upgraded>Gitプロジェクトのダウンロード</h3>
<p>今回使用するソースコードをダウンロードします。「C.gitプロジェクトをダウンロード」を実行しましょう。</p>
<p>実行すると、Google Driveの<code>Colabo NoteBooks</code>というフォルダーの中に<code>yolo-tf2-test</code>というフォルダーが自動でダウンロードされます。</p>
<p class="image-container"><img alt="img18" src="img/472d3060f74c0e91.jpeg"></p>
<p>このようにDrive内の<code>Colab Notebooks</code>フォルダー内にダウンロードされます。</p>
<h3 is-upgraded>重みのダウンロードと変換</h3>
<p>転移学習で使用する「重み」をダウンロードし変換します。ダウンロードには時間がかかります。 「D.重みのダウンロードと変換」を実行しましょう。</p>
<p>実行するとtinyモデルの重みをダウンロードします。もし通常のYOLOの重みも試してみたい方は、</p>
<pre><code>!wget https://pjreddie.com/media/files/yolov3-tiny.weights -O data/yolo_weight/yolov3-tiny.weights
!python convert.py --weights ./data/yolo_weight/yolov3-tiny.weights --output ./data/weight/yolov3-tiny.tf --tiny

# tinyを使わない場合は以下
# !wget https://pjreddie.com/media/files/yolov3.weights -O data/yolo_weight/yolov3.weights
# !python convert.py --weights ./data/yolo_weight/yolov3.weights --output ./data/weight/yolov3.tf
</code></pre>
<p>1,2行目をコメントにして、5,6行目をコメントアウトしてください。</p>
<pre><code># !wget https://pjreddie.com/media/files/yolov3-tiny.weights -O data/yolo_weight/yolov3-tiny.weights
# !python convert.py --weights ./data/yolo_weight/yolov3-tiny.weights --output ./data/weight/yolov3-tiny.tf --tiny

# tinyを使わない場合は以下
!wget https://pjreddie.com/media/files/yolov3.weights -O data/yolo_weight/yolov3.weights
!python convert.py --weights ./data/yolo_weight/yolov3.weights --output ./data/weight/yolov3.tf
</code></pre>
<p>これで事前準備はお終いです。ダウンロードには時間がかかるので、終わるのを待つ必要はありません。次のスライドに進みましょう！</p>


      </google-codelab-step>
    
      <google-codelab-step label="YOLOv3とは" duration="10">
        <p>それでは、今回試す「YOLO」について簡単に解説したいと思います。</p>
<p>難しい内容ですが、理解できなくてもプログラムを実行して物体検出を体験できますので安心してください。そういうものなのか、という程度で読み流してもらえればと思います。</p>
<p>YOLOとは、物体検出の手法の1つです。「v3」というのは、そのままバージョン3のことを表します。（※2020年5月にv4が考案されたようです）物体領域検出と画像認識を同時に行うことでより速く物体検出を行えるのが特徴です。その反面、精度は他のアルゴリズムと比べると少し落ちています。</p>
<p>まずは、YOLOがどのように物体検出を行っているのか簡単なイメージで解説します。v3での改善点は最後に解説します。</p>
<p class="image-container"><img alt="img19" src="img/3d37a0eb652f2dbf.jpeg"></p>
<p>（https://pjreddie.com/ から引用）</p>
<p>YOLOでは、まず対象の画像をいくつかのセルに分けます。この各セルに対して、「物体領域検出（どこに物体があるのか）」と「画像認識（そのセルが何なのか）」を行います。</p>
<p class="image-container"><img alt="img20" src="img/90fc609dc2fcf66e.jpeg"></p>
<p>（https://pjreddie.com/ から引用）</p>
<p>上の画像が「物体領域検出」の様子です。このように、サイズの異なる複数のバウンディングボックスを用意し、それぞれに対して「物体がどれぐらい存在しているのか」を計算します。これを「信頼度スコア」と言います。 物体が存在していなければ、信頼度スコアは0になります。</p>
<p class="image-container"><img alt="img21" src="img/47417f8171dfc598.jpeg"></p>
<p>（https://pjreddie.com/ から引用）</p>
<p>上の画像が「画像認識」の様子です。各セルに対して画像認識を行い、そのセルがどういった画像なのかを計算します。</p>
<p class="image-container"><img alt="img22" src="img/a55466ebcd8eabc8.jpeg"></p>
<p>（https://pjreddie.com/ から引用）</p>
<p>上記の「物体領域検出」と「画像認識」の結果を合わせることで、最終的に「物体検出」を行います。</p>
<p class="image-container"><img alt="img23" src="img/f672860747b2cd5e.jpeg"></p>
<p>（https://qiita.com/mdo4nt6n/items/68dcda71e90321574a2b から引用）</p>
<p>一連の流れを画像で表すとこのようになります。</p>
<h3 is-upgraded>YOLOv3での改善点</h3>
<p>上記に加えて、v3ではいくつかの工夫を追加して精度や速度の改善を行いました。</p>
<ol type="1">
<li>1/32、1/16、1/8のサイズに分けて物体検出の結果を出力します。複数のサイズに分けることで、さまざまな画像の大きさに対応可能</li>
<li>物体領域検出の際に使用するバウンディングボックスのサイズを調整</li>
</ol>
<p>以上で、YOLOについての簡単な講義は終わりです。詳細が気になる方は、Qiitaや個人のブログなどで取り上げられたりまとめられているのでそちらをご参照ください。</p>


      </google-codelab-step>
    
      <google-codelab-step label="学習データの作成" duration="10">
        <h2 is-upgraded>テスト</h2>
<p>それでは、早速学習データを作成したいと思いますが、まずは無事にダウンロードできているかテストを行います。 ノートブックに戻って、重みのダウンロードが完了していたら「E.テスト」を実行しましょう。</p>
<p class="image-container"><img alt="img24" src="img/9fc84866a84521a.jpg"></p>
<p>このように画像が出てきたらテストには問題ありません。検出が上手くいっていなくても大丈夫です。おそらくtinyのため精度が低いのでしょう。</p>
<p class="image-container"><img alt="img25" src="img/b9e35e3f269d13b9.jpg"></p>
<p>ちなみに、tinyではなく通常の重みを使用すると高精度に検出してくれます。</p>
<pre><code># tiny
# !python detect.py --weights ./data/weight/yolov3-tiny.tf --tiny --image ./data/meme.jpg

# not tiny
!python detect.py --weights ./data/weight/yolov3.tf --image ./data/meme.jpg

from IPython.display import Image,display_jpeg
display_jpeg(Image(&#39;output.jpg&#39;))
</code></pre>
<p>先ほどtinyではない重みもダウンロードした方はこちらを実行すればテストできます。</p>
<p>また、他の画像を使用することもできます。</p>
<p class="image-container"><img alt="img26" src="img/88404f68320c64f0.jpeg"></p>
<p>左のフォルダマークをクリックして、<code>drive</code> &gt; <code>My Drive</code> &gt; <code>Colab Notebooks</code> &gt; <code>yolo-tf2-test</code> &gt; <code>data</code>の順にクリックします。</p>
<p>そして、検出させたい画像を最後の<code>data</code>フォルダーの中にアップロードします。そのままドラッグアンドドロップするか赤枠のアップロードから画像を選ぶだけです。</p>
<pre><code># tiny
!python detect.py --weights ./data/weight/yolov3-tiny.tf --tiny --image ./data/meme.jpg # ← `meme.jpg` の箇所を検出したい画像ファイル名にする

# not tiny
# !python detect.py --weights ./data/weight/yolov3.tf --image ./data/meme.jpg

from IPython.display import Image,display_jpeg
display_jpeg(Image(&#39;output.jpg&#39;))
</code></pre>
<p>画像をアップロードしたら、2行目の<code>meme.jpg</code>のところをアップロードしたファイル名に変更して、再度実行すればOKです。</p>
<pre><code># tiny
!python detect.py --weights ./data/weight/yolov3-tiny.tf --tiny --image ./data/test.jpg # ← `meme.jpg` の箇所を検出したい画像ファイル名にする

# not tiny
# !python detect.py --weights ./data/weight/yolov3.tf --image ./data/meme.jpg

from IPython.display import Image,display_jpeg
display_jpeg(Image(&#39;output.jpg&#39;))
</code></pre>
<p>たとえば、<code>test.jpg</code>を対象に実行する場合、上記のように変更します。</p>
<p>それでは、学習データを作成してきます。</p>
<p>物体検出の場合、学習データは「画像」と「バウンディングボックス（正確にはバウンディングボックスの座標位置）」となります。</p>
<p>画像は、みなさんが用意したものを使用します。その画像に「VoTT」というツールを使って検出したい箇所にバウディングボックスをつけていきます（この作業をアノテーションと呼びます）。</p>
<h2 is-upgraded>オリジナルデータの解凍</h2>
<p>まずは、用意した画像を<code>original_images</code>という名前のフォルダーにまとめてください。まとめたら<code>original_images.zip</code>とzip形式に圧縮してください。</p>
<p>用意する画像のサイズや拡張子はなんでもOKです（後ほど自動でリサイズします。）。拡張子は揃えておいた方がほんの少しだけ楽です。 スマートフォンで撮った写真を使っても大丈夫です。</p>
<p class="image-container"><img alt="img26-1" src="img/e2cb5f4f66b0abc8.jpeg"></p>
<p>画像はどういったものでも問題ありませんが、始めはシンプルに検出したい物だけを映すことをオススメします。まずは、シンプルなデータで学習して物体検出を体感してみましょう。</p>
<p>圧縮したらGoogle Driveの<code>data</code>フォルダー内にアップロードします。アップロードしたら「F.オリジナルデータの解凍」を実行します。アップロードしたzipフォルダーを解凍します。</p>
<p class="image-container"><img alt="img26-2" src="img/3d8d11b3d4849d20.jpeg"></p>
<p>画像をさらにフォルダー毎に分ける必要はありません。上の画像のように、画像データだけを<code>original_images</code>フォルダーに入れてください。</p>
<h2 is-upgraded>データの整形</h2>
<p>解凍が終わったら「G.データの整形」を行い、416*416サイズに変更します。サイズ変更した画像は<code>resized_images</code>に保存されます。</p>
<p>実行する前に拡張子をプログラム内で指定する必要があります。</p>
<pre><code>extension = &#39;jpg&#39;
</code></pre>
<p>8行目の<code>jpg</code>の箇所を皆さんのデータの拡張子に揃えてください。</p>
<p>トリミングではなく、1:1となるようにリサイズし足りない箇所は白色で埋めるようにしています。</p>
<p>複数の拡張子が混在している場合は、お手数ですがすべての拡張子に対してこの作業をお願いします。</p>
<h2 is-upgraded>画像の水増し</h2>
<p>次に画像の水増しを行います。</p>
<pre><code>############
volume = 60 # 1枚あたりどれだけ増やすか
img_name = &#39;img&#39; # 生成された画像のファイル名
############
</code></pre>
<p>この箇所の数値を変更することで、画像1枚に対して水増しする量を調整できます。上記の例の場合、1枚に対しておよそ60枚複製されることになります。</p>
<p>枚数を決定したら「H.画像の水増し」を実行しましょう。最初は1クラスに対して合計100枚ぐらいになるように設定することをオススメします。一度簡単に試してみて上手く検出できていたら画像を増やしていく方が効率的です。</p>
<p>また、新たに画像を追加する場合は、<code>img_name = &#39;img&#39;</code>の<code>img</code>部分を変更して、水増しされた画像のファイル名が被らないようにしましょう。（言ってることがわかりにくい方は、実際に水増しされたあとの画像ファイル名を確認してみるとわかりやすいです）</p>
<pre><code>datagen = ImageDataGenerator(
    channel_shift_range=30,
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.05,
    height_shift_range=0.1,
    )
</code></pre>
<p>画像の水増しは、ただ複製するだけではなく少し加工した上で複製を行います。どのように加工するか指定するパラメーターについては、<a href="https://qiita.com/takurooo/items/c06365dd43914c253240" target="_blank">こちら</a>で詳しく解説されていますのでご参照ください。</p>
<p>ここも調整することで精度が変化するので各自色々と試してみましょう。</p>
<h2 is-upgraded>変形した画像を圧縮してダウンロード</h2>
<p>データを水増しができたら最後に「I.変形した画像を圧縮してダウンロード」を実行します。実行すると自動でダウンロードされますが、たまに、ポップアップが出現しダウンロードの許可ボタンを押す必要がありますので注意してください。</p>
<p>これで画像を用意できたので、次はアノテーションを行っています。</p>
<p>まずは、VoTTを<a href="https://github.com/Microsoft/VoTT/releases" target="_blank">こちら</a>からダウンロードしてください。</p>
<p class="image-container"><img alt="img27" src="img/615f7c538c943281.jpeg"></p>
<p>自分のOSに合わせて<code>.dmg</code>,<code>.snap</code>,<code>.exe</code>ファイルのどれかをダウンロードしてください。ダウンロードしたらファイルを実行して、VoTTを立ち上げましょう。</p>
<p class="image-container"><img alt="img28" src="img/3b9ac9264bf60edd.jpeg"></p>
<p>立ち上がったら<code>New Project</code>を選びます。</p>
<p class="image-container"><img alt="img29" src="img/5a78357d45846bf1.jpeg"></p>
<p>プロジェクトの名前などを設定します。</p>
<ul>
<li>Display Name: プロジェクトの名前を記入</li>
<li>Security Token: とくに設定不要</li>
<li>Source Connection: アノテーションする画像フォルダーを選ぶ</li>
<li>Target Connection: データの出力先を選ぶ</li>
<li>Description: とくに設定不要</li>
<li>Video Settings: 動画のフレーム数を設定、今回は不要</li>
<li>Tags: タグの作成</li>
</ul>
<p>Tagsに検出したい画像の名前を追加してください。</p>
<p>Source / Target Connectionは、まずは<code>Add Connection</code>から新しく作成する必要があります。</p>
<p class="image-container"><img alt="img30" src="img/bbc07f82f9dede8f.jpeg"></p>
<ul>
<li>Display Name: Connectionimの名前</li>
<li>Provider: Local File Systemを選ぶ</li>
<li>Local File System: ローカル内の任意のディレクトリを選ぶ</li>
</ul>
<p>出力先のフォルダーは、別途<code>out</code>のような名前のフォルダーを追加するとわかりやすいです。</p>
<p class="image-container"><img alt="img31" src="img/f10698dad50bd431.jpeg"></p>
<p>このようにダウンロードしてきた<code>changed_images</code>をSource　Connectionに設定し、その中に<code>out</code>を作ってTarget Connectionにするのがお勧めです。</p>
<p>それぞれ設定できたら<code>Save Connection</code>をクリックします。これでアノテーションに入れますが、もう少し設定を変更します。</p>
<p class="image-container"><img alt="img32" src="img/4d6bf12f49c50f97.jpeg"></p>
<p>まずは、左の上から4つ目のアイコンをクリックして、出力形式を上記画像のように変更します。</p>
<ul>
<li>Provider: Pascal VOC</li>
<li>Asset State: Only tagged Assets</li>
<li>Test / Train Split: 100</li>
</ul>
<p>変更したら、<code>Save Export Settings</code>をクリックします。</p>
<p class="image-container"><img alt="img33" src="img/f4d00b33fd0c9911.jpeg"></p>
<p>次に、左の上から5番目のアイコンをクリックして、上記のように設定します。VoTTの機能でアノテーションを補助してくれます。ここでは、自動でバウンディングボックスをつけるようにしています。不要なバウンディングボックスが多ければ、<code>Auto Detect</code>のチェックマークを外してください。</p>
<p>変更したら、<code>Save Project</code>をクリックします。</p>
<p>これで設定はお終いです。左の上から2番目をクリックしてアノテーション作業に入ります。</p>
<p class="image-container"><img alt="img34" src="img/a6811163bd6ed604.jpg"></p>
<p>直感的に操作できるUIですが、簡単に操作説明をします。</p>
<ol type="1">
<li>アノテーション用のボックスを作成</li>
<li>前/次の画像に移動</li>
<li>保存</li>
<li>データの出力</li>
<li>タグの選択</li>
</ol>
<p>基本的に1 → 5 → 2の流れで行います。全部アノテーションしたら、4をクリックしてください。</p>
<p>その他以下の機能があります。</p>
<ul>
<li>タグは鍵マークをクリックすると固定になり自動でそのタグがつく</li>
<li>タグの右側にある数字キーを押すとそのタグがつく</li>
<li>バウンディングボックスは重なっていても問題ない</li>
<li>タグの左側をクリックした後に、鉛筆ボタンをクリックすると色を選べる（どの色を使っても問題ない）</li>
<li>Wキーを押すと前の画像に移動</li>
<li>Sキーを押すと次の画像に移動</li>
</ul>
<p>以上が、VoTTの使い方です。細かな使い方や機能は各自で調べてもらえればと思います。慣れると単調な作業になり辛いですが、根気よく行うしかありません。</p>


      </google-codelab-step>
    
      <google-codelab-step label="学習" duration="10">
        <p>次に学習を実行します。画像の量に応じて、「画像の解凍」「tfrecordへの変換」「学習」それぞれに時間がかかります。</p>
<h2 is-upgraded>画像の解凍</h2>
<p>アノテーション作業が終わったら、VoTTから必要なデータを出力します。出力ボタンを押すだけで、Target Connectionで設定したフォルダーにデータが出力されます。</p>
<p class="image-container"><img alt="img35" src="img/5b1f920030068f40.jpeg"></p>
<p>右上にこのようなポップが表示されたら無事出力成功です。</p>
<p class="image-container"><img alt="img36" src="img/d2731d25caca9ddd.jpeg"></p>
<p>このようなポップが出てきた場合は、失敗です。原因不明でたまに失敗します。 プロジェクトを保存して、一度VoTTを閉じて再度起動して出力ボタンを再度押してください。また、失敗したらこれを繰り返してください。</p>
<p class="image-container"><img alt="img37" src="img/88b676a3a68cc26a.jpeg"></p>
<p>Target Connectionには、このようにjsonデータが大量に生成されています。今回必要なのは<code>~~~~~ -PascalVOC-export</code>というフォルダーです（変更日など時間でソートをすると見つけやすいです。）。</p>
<p class="image-container"><img alt="img38" src="img/be17770cebcf72fd.jpeg"></p>
<p>このフォルダーを<code>train_images</code>に名前を変えて、zip形式に圧縮します。圧縮したら、Google Driveにアップロードします。アップロード先は、同様に<code>data</code>フォルダー内です。</p>
<p>アップロードできたら「J.画像の解凍」を実行してください。</p>
<h2 is-upgraded>tfrecordの作成</h2>
<p>次に用意したデータを「tfrecord」という形式に変換します。tfrecordとは、tensorflowが推奨する軽量なデータフォーマットです。</p>
<p>大量の学習データを1つにまとめていきます。また、合わせてデータの水増しも行っています。</p>
<p>「K.tfrecordの作成」を実行する前に、9~11行目を変更してください。</p>
<ul>
<li><code>classes_list</code>: VoTTで使用したタグ名を記載（順番は関係なし）</li>
<li><code>tf_file_name</code>: tfrecordのファイル名</li>
<li><code>augment_size</code>: 画像の水増しの量</li>
</ul>
<pre><code>classes_list = [&#39;rock&#39;, &#39;scisor&#39;, &#39;paper&#39;] # タグ(順番はVoTTと一致していなくても大丈夫)
tf_file_name = &#39;hand&#39; # ファイル名
augment_size = 1 # 画像の水増し量
</code></pre>
<p>このような感じで変更してください。 水増し量は、各画像に対して何枚増やすかになります。そのため、数枚程度に留めておいた方が良いと思います。</p>
<p>設定したら「K.tfrecordの作成」を実行してください。</p>
<p class="image-container"><img alt="img39" src="img/7cd5768231676e8f.jpeg"></p>
<p>実行すると<code>data</code>フォルダに<code>train</code>,<code>val</code>,<code>test</code>の3つのtfrecordが生成されます。ファイル名は、<code>tf_file_name</code>で指定した文字列 + <code>_train</code>,<code>_val</code>,<code>_test</code>となります。</p>
<p>それぞれ訓練用、検証用、テスト用です。</p>
<h2 is-upgraded>tfrecordの表示</h2>
<p>tfrecordを作成できたら、テストとして表示させてみます。「L.tfrecordの表示」を実行する前に、14,15行目を設定します。</p>
<pre><code>#######################
tfrecord_name = &#39;hand&#39; # tfrecordの名前
tfrecord_kind = &#39;train&#39; # tfrecotdの種類, train, val, test のどれか
#######################
</code></pre>
<p><code>tfrecord_name</code>は表示させたいtfrecodのファイル名を、<code>tfrecord_lind</code>にはtrain,val,testのどれかを設定します。</p>
<p>設定したら「L.tfrecordの表示」を実行しましょう。</p>
<p>バウンディングボックスのついた画像が表示されると思います。</p>
<h2 is-upgraded>学習</h2>
<p>これで学習データの準備ができました。次に学習を行っていきます。</p>
<p>「M.学習」を実行する前に、25,26行目を変更してください。</p>
<pre><code>###################################
your_weight_name = &#39;cola_tea_monster_test&#39;
tfrecord_name = &#39;cola_tea_monster&#39;
###################################
</code></pre>
<p>保存する重みのファイル名、学習の際に使用するtfrecordの名前を設定します。設定したら「M.学習」を実行しましょう。画像の量によって学習にかかる時間は変わりますが、100~200枚程度であれば30~60分で終わると思います。</p>
<p>また、ディープラーニングのハイパーパラメーターは、30~34行目で設定できます。</p>
<pre><code>###################################
epochs = 100
batch_size = 16
learning_rate = 1e-3 # 0.001
tiny = True
###################################
</code></pre>
<p>tinyを<code>False</code>にすると通常のYOLOv3の重みで転移学習を行うことができます（もちろん、事前に重みのダウンロードは必要です）。</p>
<p>EaryStoppingを適用しているので、過学習が発生している場合には上記で設定した<code>epochs</code>分学習しない場合もあります。</p>


      </google-codelab-step>
    
      <google-codelab-step label="物体検出１" duration="10">
        <p>学習している間にラズパイでの物体検出を体感してもらいます。こちらで用意したジャンケンの手を検出する重みを使用します。</p>
<p>まずは、ラズパイを起動してターミナルを起動します。 <img alt="img40" src="img/4a9d68c7d025faae.jpeg"></p>
<p>ラズパイのホーム画面の左上の赤枠をクリックするか<code>Ctrl</code>+<code>Alt</code>+<code>t</code>のショートカットキーでも起動できます。</p>
<p>起動したら必要なものをインストールします。パスワードの入力を求められたら、入力してください。</p>
<p><code>sudo apt update</code></p>
<p><code>sudo apt upgrade -y</code></p>
<p><code>sudo apt install python3-dev python3-pip libatlas-base-dev</code></p>
<p>すべて終わったら、<code>cd Desktop</code>でデスクトップに移動し、Gitプロジェクトwダウンロードします。</p>
<p><code>git clone https://jw-fujibayashi:moyashi314%40tmge@github.com/jw-fujibayashi/yolo-tf2-test.git</code></p>
<p>クローンしたら<code>cd yolo-tf2-test</code>を実行し、また少し設定を行います。</p>
<p><code>wget &#34;https://raw.githubusercontent.com/PINTO0309/Tensorflow-bin/master/tensorflow-2.2.0-cp37-cp37m-linux_armv7l_download.sh&#34;</code></p>
<p><code>chmod 764 ./tensorflow-2.2.0-cp37-cp37m-linux_armv7l_download.sh</code></p>
<p><code>./tensorflow-2.2.0-cp37-cp37m-linux_armv7l_download.sh</code></p>
<p><code>sudo pip3 install tensorflow-2.2.0-cp37-cp37m-linux_armv7l.whl</code></p>
<p><code>sudo pip3 install -r requirements.txt</code></p>
<p>無事ダウンロードできたら、<a href="https://drive.google.com/open?id=1FvNGTYcpmRwvAkKUUwK4YzLDGmlfUPPs" target="_blank">こちら</a>の重みをダウンロードしてください。3ファイルありますが、3ファイルともダウンロードしてください。</p>
<p>ダウンロードできたら、ラズパイの<code>yolov2-tf-test</code>の<code>data</code>の<code>weight</code>フォルダーに入れます。</p>
<pre><code>python3 detect_video.py --video 0 --tiny --weights data/weight/hands.tf
</code></pre>
<p>最後に上記コマンドを実行すると、リアルタイム物体検出のプログラムが実行します。ラズパイにカメラをつけた状態で実行する必要があるので注意してください。</p>
<p>プログラムを終了する際には、<code>q</code>キーを押してください。</p>


      </google-codelab-step>
    
      <google-codelab-step label="物体検出2" duration="10">
        <h2 is-upgraded>Google Colabo上でテスト</h2>
<p>無事に学習が終わったら、でき上がった重みを使って物体検出を行っていきます。まずは、Goolge Colaboratory上でテストをしてみます。</p>
<p>「N.Google Colabo上でテスト」を実行する前に、16,17行目を変更してください。</p>
<pre><code>###################################
your_weight_name = &#39;&#39;
tfrecord_name = &#39;&#39;
###################################
</code></pre>
<p><code>your_weight_name</code>にさっきの学習でできた重みファイルの名前を、<code>tfrecord_name</code>に使用したtfrecordの名前を設定します（test用のtfrecordを使用します。<code>_test</code>は不要です。）。</p>
<p>設定したら「N.Google Colabo上でテスト」を実行してください。上手く検出できたでしょうか？納得のいかない精度の場合は、学習データを増やしたりしてみましょう。（学習のコツは次のスライドでまとめています。）</p>
<p>それでは、最後にラズパイでリアルタイム物体検出をしてみましょう。まずは、作成した「重みファイル3つ（<code>data</code>&gt;<code>weight</code>フォルダーにあります。）」と「<code>classes.txt</code>（<code>data</code>フォルダーにあります。）」をダウンロードします。</p>
<p>重みファイルは、<code>data</code>の中の<code>weight</code>フォルダーに移動させましょう。 <code>classes.txt</code>は、<code>data</code>フォルダーに移動させます。既にある<code>classes.txt</code>は上書きしても大丈夫です。</p>
<p>移動させたらターミナルを開いて、<code>cd Desktop/yolo-tf2-master</code>を実行して</p>
<pre><code>python detect_video.py --video 0 --tiny --weights data/weight/{各自の重みファイル名}.tf
</code></pre>
<p>を実行します。重みファイルは、<code>~~~.tf.index</code>,<code>~~~.data-~~~</code>などのようになっていると思いますが、<code>.tf</code>までで指定すれば大丈夫です。</p>


      </google-codelab-step>
    
      <google-codelab-step label="学習のコツ" duration="0">
        <p>自分が試してみて上手くいったと感じたコツを以下にまとめます。論理的な部分もない場合もありますので、軽く参考にしてもらえればと思います。</p>
<p>また、一般的に高精度の物体検出を行うには、1クラスあたり1000枚の画像が必要されると言われています。（単純に水増しして1000枚に達すれば良いわけではありません）</p>
<p>それと比べると今回は圧倒的にデータ量が足りていなく、かつtinyで学習しているので精度に限界があることは念頭に置いておいてください。</p>
<h2 is-upgraded>学習データは量よりも種類</h2>
<p>似たような画像をたくさん用意するよりも、バリエーションを増やした方が精度は向上します。たとえば、背景を変えるだけでも大丈夫です。また、遠くから撮った画像なども用意するとより精度は向上します。</p>
<p>その他、角度/光加減などを変えた画像を用意するとなお良くなります。</p>
<h2 is-upgraded>転移学習を活かす</h2>
<p>今回、転移学習に使っている重みは「coco」と呼ばれるデータセットを使用して学習された重みです。</p>
<ul>
<li>person</li>
<li>bicycle</li>
<li>car</li>
<li>motorbike</li>
<li>aeroplane</li>
<li>bus</li>
<li>train</li>
<li>truck</li>
<li>boat</li>
<li>traffic light</li>
<li>fire hydrant</li>
<li>stop sign</li>
<li>parking meter</li>
<li>bench</li>
<li>bird</li>
<li>cat</li>
<li>dog</li>
<li>horse</li>
<li>sheep</li>
<li>cow</li>
<li>elephant</li>
<li>bear</li>
<li>zebra</li>
<li>giraffe</li>
<li>backpack</li>
<li>umbrella</li>
<li>handbag</li>
<li>tie</li>
<li>suitcase</li>
<li>frisbee</li>
<li>skis</li>
<li>snowboard</li>
<li>sports ball</li>
<li>kite</li>
<li>baseball bat</li>
<li>baseball glove</li>
<li>skateboard</li>
<li>surfboard</li>
<li>tennis racket</li>
<li>bottle</li>
<li>wine glass</li>
<li>cup</li>
<li>fork</li>
<li>knife</li>
<li>spoon</li>
<li>bowl</li>
<li>banana</li>
<li>apple</li>
<li>sandwich</li>
<li>orange</li>
<li>broccoli</li>
<li>carrot</li>
<li>hot dog</li>
<li>pizza</li>
<li>donut</li>
<li>cake</li>
<li>chair</li>
<li>sofa</li>
<li>pottedplant</li>
<li>bed</li>
<li>diningtable</li>
<li>toilet</li>
<li>tvmonitor</li>
<li>laptop</li>
<li>mouse</li>
<li>remote</li>
<li>keyboard</li>
<li>cell phone</li>
<li>microwave</li>
<li>oven</li>
<li>toaster</li>
<li>sink</li>
<li>refrigerator</li>
<li>book</li>
<li>clock</li>
<li>vase</li>
<li>scissors</li>
<li>teddy bear</li>
<li>hair drier</li>
<li>toothbrush</li>
</ul>
<p>たとえば、ペットボトルのお茶とコカコーラの物体検出は、比較的少ない画像データで精度が高くなりました。おそらく、bottoleが事前に学習されていたからだと思います。</p>
<h2 is-upgraded>推論時と同じ環境のデータを用意する</h2>
<p>どんな時でも物体検出を高精度で行う重みを作成するのは大変です。そのため、AIが推論を上手く行えるように寄り添ってあげる必要があります。</p>
<p>その1つとして、学習データを推論時に映る風景を合わせてあげると精度が上ります。</p>
<p>たとえば、防犯カメラのような定点カメラとして使用するのであれば、その風景でのみ物体検出できれば良いので学習データを合わせてあげると良いと思います。</p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
